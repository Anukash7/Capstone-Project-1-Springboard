{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# import pyspark\n",
    "# from pyspark.sql import SQLContext\n",
    "# from pyspark import SparkContext    \n",
    "\n",
    "# sc = SparkContext(\"local\", \"First App\")\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"First App\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\anuka\\anaconda3\\lib\\site-packages (0.90)\n",
      "Requirement already satisfied: scipy in c:\\users\\anuka\\anaconda3\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\anuka\\anaconda3\\lib\\site-packages (from xgboost) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install researchpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\anuka\\\\Anaconda3\\\\Springboard_Capstone_Project_1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import itertools as it\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: black;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adds style to the printed dataframe output\n",
    "from IPython.core.display import HTML\n",
    "css = open('C:/Users/anuka/OneDrive/Desktop/Anu/Interview Prep/Springboard/Curriculum/Unit 5/Pandas from the ground up/pycon-pandas-tutorial-master/pycon-pandas-tutorial-master/style-table.css').read() + open('C:/Users/anuka/OneDrive/Desktop/Anu/Interview Prep/Springboard/Curriculum/Unit 5/Pandas from the ground up/pycon-pandas-tutorial-master/pycon-pandas-tutorial-master/style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training dataset is:  (213451, 16)\n",
      "The shape of test dataset is:  (62096, 15)\n",
      "The shape of sessions dataset is:  (10567737, 6)\n",
      "The shape of countries dataset is:  (10, 7)\n",
      "The shape of age_gender_buckets dataset is:  (420, 5)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/anuka/OneDrive/Desktop/Anu/Interview Prep/Springboard/Curriculum/Capstone project 1/airbnb-recruiting-new-user-bookings\")\n",
    "\n",
    "# Load and clean training dataset\n",
    "df_train = pd.read_csv(\"train_users_2.csv\",parse_dates=['timestamp_first_active','date_account_created','date_first_booking'], skiprows=0, na_values = ['NaN'], encoding='utf-8')\n",
    "\n",
    "# Load and clean test dataset\n",
    "df_test = pd.read_csv(\"test_users.csv\",parse_dates=['timestamp_first_active','date_account_created','date_first_booking'], skiprows=0, na_values = ['NaN'])\n",
    "\n",
    "# Load and clean sessions dataset\n",
    "df_sessions = pd.read_csv(\"sessions.csv\",parse_dates=True, skiprows=0, na_values = ['NaN'])\n",
    "\n",
    "# Load and clean countries dataset\n",
    "df_countries = pd.read_csv(\"countries.csv\",parse_dates=True, skiprows=0, na_values = ['NaN'])\n",
    "\n",
    "# Load and clean age_gender_bkts dataset\n",
    "# This dataset contains number of male/female in different age buckets who travelled to these destination countries in 2015 \n",
    "df_age_gender_bkts = pd.read_csv(\"age_gender_bkts.csv\",parse_dates=True, skiprows=0, na_values = ['NaN'])\n",
    "\n",
    "# Get shape of all input datasets\n",
    "print(\"The shape of training dataset is: \", df_train.shape)\n",
    "print(\"The shape of test dataset is: \", df_test.shape)\n",
    "print(\"The shape of sessions dataset is: \", df_sessions.shape)\n",
    "print(\"The shape of countries dataset is: \", df_countries.shape)\n",
    "print(\"The shape of age_gender_buckets dataset is: \", df_age_gender_bkts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_account_created</th>\n",
       "      <th>timestamp_first_active</th>\n",
       "      <th>date_first_booking</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>signup_method</th>\n",
       "      <th>signup_flow</th>\n",
       "      <th>language</th>\n",
       "      <th>affiliate_channel</th>\n",
       "      <th>affiliate_provider</th>\n",
       "      <th>first_affiliate_tracked</th>\n",
       "      <th>signup_app</th>\n",
       "      <th>first_device_type</th>\n",
       "      <th>first_browser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-07-01 00:00:06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>35.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Moweb</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Mobile Safari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jtl0dijy2j</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-07-01 00:00:51</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Moweb</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Mobile Safari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xx0ulgorjt</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-07-01 00:01:48</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>linked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6c6puo6ix0</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-07-01 00:02:15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>linked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>czqhjk3yfe</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-07-01 00:03:05</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Safari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id date_account_created timestamp_first_active date_first_booking  \\\n",
       "0  5uwns89zht           2014-07-01    2014-07-01 00:00:06                NaT   \n",
       "1  jtl0dijy2j           2014-07-01    2014-07-01 00:00:51                NaT   \n",
       "2  xx0ulgorjt           2014-07-01    2014-07-01 00:01:48                NaT   \n",
       "3  6c6puo6ix0           2014-07-01    2014-07-01 00:02:15                NaT   \n",
       "4  czqhjk3yfe           2014-07-01    2014-07-01 00:03:05                NaT   \n",
       "\n",
       "      gender   age signup_method  signup_flow language affiliate_channel  \\\n",
       "0     FEMALE  35.0      facebook            0       en            direct   \n",
       "1  -unknown-   NaN         basic            0       en            direct   \n",
       "2  -unknown-   NaN         basic            0       en            direct   \n",
       "3  -unknown-   NaN         basic            0       en            direct   \n",
       "4  -unknown-   NaN         basic            0       en            direct   \n",
       "\n",
       "  affiliate_provider first_affiliate_tracked signup_app first_device_type  \\\n",
       "0             direct               untracked      Moweb            iPhone   \n",
       "1             direct               untracked      Moweb            iPhone   \n",
       "2             direct                  linked        Web   Windows Desktop   \n",
       "3             direct                  linked        Web   Windows Desktop   \n",
       "4             direct               untracked        Web       Mac Desktop   \n",
       "\n",
       "   first_browser  \n",
       "0  Mobile Safari  \n",
       "1  Mobile Safari  \n",
       "2         Chrome  \n",
       "3             IE  \n",
       "4         Safari  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup and Derived Features\n",
    "1. Concatenate train and test data so that cleaning steps need to be done only once\n",
    "2. Drop date_first_booking which is completely missing in the test data\n",
    "3. Replace unknown values in gender and first_browser to NaN\n",
    "4. Age will be filtered between 18-100 and set to NaN\n",
    "5. Date time data of 'time_first_active' and 'timestamp_first_active' to be split into day, month and year columns\n",
    "6. Drop date_account_created and timestamp_first_active columns\n",
    "7. Remove leading and trailing spaces from language column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275547, 20)\n",
      "(10567737, 6)\n",
      "(135483, 5)\n",
      "(275547, 24)\n",
      "id                             0\n",
      "gender                         0\n",
      "age                            0\n",
      "signup_method                  0\n",
      "signup_flow                    0\n",
      "language                       0\n",
      "affiliate_channel              0\n",
      "affiliate_provider             0\n",
      "first_affiliate_tracked        0\n",
      "signup_app                     0\n",
      "first_device_type              0\n",
      "first_browser                  0\n",
      "country_destination        62096\n",
      "type                           0\n",
      "dac_year                       0\n",
      "dac_month                      0\n",
      "dac_day                        0\n",
      "tfa_year                       0\n",
      "tfa_month                      0\n",
      "tfa_day                        0\n",
      "cnt_action                     0\n",
      "cnt_uniq_action_type           0\n",
      "cnt_uniq_dev_type              0\n",
      "secs_per_session               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combining the test and train data. So that all the cleaning steps can be combined\n",
    "\n",
    "test_ids = df_test['id']\n",
    "\n",
    "df_train['type'] = 'train'\n",
    "df_test['type'] = 'test'\n",
    "\n",
    "df_all = pd.concat((df_train, df_test), axis = 0, ignore_index = True)\n",
    "\n",
    "df_all = df_all.drop(['date_first_booking'], axis=1)\n",
    "\n",
    "df_all.gender.replace('-unknown-', np.nan, inplace=True)\n",
    "df_all.first_browser.replace('-unknown-', np.nan, inplace=True)\n",
    "\n",
    "df_all.loc[df_all.age > 100, 'age'] = np.nan\n",
    "df_all.loc[df_all.age < 18, 'age'] = np.nan\n",
    "\n",
    "# Splitting date time data for date account created\n",
    "df_all['dac_year'] = df_all.date_account_created.dt.year\n",
    "df_all['dac_month'] = df_all.date_account_created.dt.month\n",
    "df_all['dac_day'] = df_all.date_account_created.dt.day\n",
    "\n",
    "# Splitting date time data for time first active\n",
    "df_all['tfa_year'] = df_all.timestamp_first_active.dt.year\n",
    "df_all['tfa_month'] = df_all.timestamp_first_active.dt.month\n",
    "df_all['tfa_day'] = df_all.timestamp_first_active.dt.day\n",
    "\n",
    "df_all.drop('date_account_created',1, inplace=True)\n",
    "df_all.drop('timestamp_first_active',1, inplace=True)\n",
    "\n",
    "df_all.language = df_all.language.str.strip()\n",
    "\n",
    "print(df_all.shape)\n",
    "\n",
    "\n",
    "# Aggregate sessions dataset at user_id level to merge with rest of the data\n",
    "df_sessions_agg = df_sessions.groupby('user_id').agg({'action': 'count', 'action_type':'nunique','device_type':'nunique','secs_elapsed':np.mean}).reset_index()\n",
    "df_sessions_agg.user_id = df_sessions_agg.user_id.str.strip()\n",
    "df_sessions_agg.columns = ['user_id','cnt_action','cnt_uniq_action_type','cnt_uniq_dev_type','secs_per_session']\n",
    "print(df_sessions.shape)\n",
    "print(df_sessions_agg.shape)\n",
    "\n",
    "df_all.id = df_all.id.str.strip()\n",
    "df_all_1 = df_all.merge(df_sessions_agg, how = 'left', left_on=['id'],right_on = ['user_id'])\n",
    "df_all_1.drop(['user_id'], axis = 1, inplace=True)\n",
    "print(df_all_1.shape)\n",
    "\n",
    "# Impute missing values\n",
    "categorical = ['gender', 'first_affiliate_tracked', 'first_browser']\n",
    "numeric = ['age','cnt_action','cnt_uniq_action_type','cnt_uniq_dev_type','secs_per_session']\n",
    "impute_values = {}\n",
    "for col in numeric:\n",
    "    try:\n",
    "        impute_values[col] = df_all_1[col].median()\n",
    "    except:\n",
    "         pass\n",
    "for col in categorical:\n",
    "    try:\n",
    "        impute_values[col] = 'Missing'\n",
    "    except:\n",
    "        pass\n",
    "df_all_1 = df_all_1.fillna(impute_values)\n",
    "print(df_all_1.isnull().sum())\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_all_1['country_dest_cd'] = le.fit_transform(df_all_1['country_destination'].astype(str))\n",
    "selected_cols = ['gender', 'age', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', \n",
    "                 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser', 'dac_year', 'dac_month', \n",
    "                 'dac_day', 'tfa_year', 'tfa_month', 'tfa_day', 'cnt_action', 'cnt_uniq_action_type', 'cnt_uniq_dev_type', \n",
    "                 'secs_per_session']\n",
    "X = pd.get_dummies(df_all_1[df_all_1.type == 'train'][selected_cols])\n",
    "y = df_all_1[df_all_1.type == 'train']['country_dest_cd']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "# y_train_bal.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate(y,y_hat,labels):\n",
    "  print(classification_report(y,y_hat))\n",
    "  cm = confusion_matrix(y,y_hat)\n",
    "  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "  cmat = pd.DataFrame(cm)\n",
    "  cmat.columns = labels\n",
    "  cmat.set_index([pd.Index(labels, '')],inplace=True)\n",
    "  sns.heatmap(cmat,cmap=\"YlGnBu\", annot=True)\n",
    "  plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. XGBoost Classifier\n",
    "\n",
    "A great source to learn how Boosting works:\n",
    "https://campus.datacamp.com/courses/extreme-gradient-boosting-with-xgboost/classification-with-xgboost?ex=8\n",
    "\n",
    "XGBoost algorithm = Ensemble learning algorithm as it produces the output of many models for final prediction\n",
    "1. Popular because of its speed and performance\n",
    "2. Core algorithm is parallelizable\n",
    "3. Consistently outperforms single algorithm method in ML competition\n",
    "4. State of the art performance in many ML tasks - Mainly due to Boosting\n",
    "\n",
    "Prediction in decision trees happens at the leaves of the tree\n",
    "\n",
    "Boosting:\n",
    "1. Not a specific machine learning algorithm\n",
    "2. It's a meta algorithm that can be applied to a set of machine learning models\n",
    "3. Ensemble meta algorithm used to convert many weak learners into a strong learner\n",
    "\n",
    "Weak learner - ML algorithm that is slighly better than chance - decison tree whose predictions are slightly better than 50%\n",
    "Strong learner - Any algorithm that can be tuned to achieve good performance\n",
    "\n",
    "When to use XGBoost?\n",
    "1. you have large number of training samples - greater than 1000 training samples and less than 100 features\n",
    "2. Number of features < # training samples\n",
    "3. You have a mix of categorical and numeric features or just numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not run as was taking too long to run\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import numpy as np\n",
    "# xgb = XGBClassifier(objective = 'multi:softprob', num_class = 12)\n",
    "# hyper_params = {'n_estimators':[20,50], 'max_depth':[7], 'learning_rate': [0.3, 0.5], 'n_jobs':[10]} #, 'colsample_bynode': [1], }\n",
    "# fitmodel = GridSearchCV(xgb, param_grid=hyper_params, cv=5, scoring=\"accuracy\")\n",
    "# fitmodel.fit(X_train_bal, y_train_bal)\n",
    "# print(fitmodel.best_estimator_)\n",
    "# print(fitmodel.best_params_)\n",
    "# print(fitmodel.best_score_)\n",
    "# print(accuracy_score(fitmodel.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score for train dataset 0.6542984305457953\n",
      "Accuracy_score for test dataset 0.6422949590855144\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=50, objective='multi:softprob')\n",
    "xgb.fit(X_train, y_train)\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy_score for train dataset\", accuracy_score(y_train_pred, y_train))\n",
    "print(\"Accuracy_score for test dataset\", accuracy_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)\n",
    "len(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hold data size:  64036\n"
     ]
    }
   ],
   "source": [
    "variables = X_train.columns.tolist()\n",
    "X_hold = pd.get_dummies(df_all_1[df_all_1.type == 'test'][selected_cols]) # the hold out or test set (as per Kaggle) for predictions\n",
    "\n",
    "hold_cols = set(X_hold.columns)\n",
    "columns_not_present = set(variables) - hold_cols\n",
    "for col in columns_not_present:\n",
    "    X_hold[col] = 0\n",
    "X_hold = X_hold[variables]\n",
    "print('hold data size: ', X_test.shape[0])\n",
    "        \n",
    "y_hold_pred = xgb.predict_proba(X_hold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "      <th>Variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.147144</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.124529</td>\n",
       "      <td>first_browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.120520</td>\n",
       "      <td>signup_method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.104899</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.080544</td>\n",
       "      <td>first_affiliate_tracked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.080057</td>\n",
       "      <td>affiliate_channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070542</td>\n",
       "      <td>affiliate_provider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.062994</td>\n",
       "      <td>tfa_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.052968</td>\n",
       "      <td>first_device_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050710</td>\n",
       "      <td>cnt_uniq_action_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.038557</td>\n",
       "      <td>signup_app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.016230</td>\n",
       "      <td>signup_flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011970</td>\n",
       "      <td>dac_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.007041</td>\n",
       "      <td>tfa_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.006150</td>\n",
       "      <td>secs_per_session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006010</td>\n",
       "      <td>cnt_uniq_dev_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005854</td>\n",
       "      <td>cnt_action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005042</td>\n",
       "      <td>dac_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.004286</td>\n",
       "      <td>tfa_day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003953</td>\n",
       "      <td>dac_day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Importance                 Variable\n",
       "12    0.147144                   gender\n",
       "10    0.124529            first_browser\n",
       "16    0.120520            signup_method\n",
       "2     0.104899                      age\n",
       "9     0.080544  first_affiliate_tracked\n",
       "0     0.080057        affiliate_channel\n",
       "1     0.070542       affiliate_provider\n",
       "19    0.062994                 tfa_year\n",
       "11    0.052968        first_device_type\n",
       "4     0.050710     cnt_uniq_action_type\n",
       "14    0.038557               signup_app\n",
       "15    0.016230              signup_flow\n",
       "8     0.011970                 dac_year\n",
       "18    0.007041                tfa_month\n",
       "13    0.006150         secs_per_session\n",
       "5     0.006010        cnt_uniq_dev_type\n",
       "3     0.005854               cnt_action\n",
       "7     0.005042                dac_month\n",
       "17    0.004286                  tfa_day\n",
       "6     0.003953                  dac_day"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_all = ['gender', 'age', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider',\n",
    "                   'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "def de_one_hot_encode(s):\n",
    "    for col in categorical_all:\n",
    "        if(col in s):\n",
    "            return col\n",
    "    return s\n",
    "\n",
    "# Get variable importance\n",
    "variables = X_train.columns.tolist()\n",
    "features = pd.DataFrame({'Importance':xgb.feature_importances_ , 'Variable':variables})\n",
    "features['Variable'] = features['Variable'].apply(de_one_hot_encode)\n",
    "features = features.groupby('Variable').sum().reset_index()\n",
    "features = features.sort_values('Importance', ascending=False )\n",
    "features = features[['Importance', 'Variable']]\n",
    "display(features)\n",
    "features.to_csv('feature_importances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[    0     0     0     0     0     0     0    88     0     0    83     0]\n",
      " [    0     0     0     0     0     0     0   217     0     0   203     0]\n",
      " [    0     0     0     0     0     0     0   181     0     0   153     0]\n",
      " [    0     0     0     0     0     0     0   375     0     0   311     0]\n",
      " [    0     0     0     0     0     0     0   819     0     0   686     0]\n",
      " [    0     0     0     0     0     0     0   405     0     0   343     0]\n",
      " [    0     0     0     0     0     0     0   487     0     0   379     0]\n",
      " [    0     0     1     0     0     0     1 32199     1     0  5000     4]\n",
      " [    0     0     0     0     0     0     0   109     0     0   116     0]\n",
      " [    0     0     0     0     0     0     0    42     0     1    34     0]\n",
      " [    0     0     1     0     0     0     2  9850     0     0  8929     3]\n",
      " [    0     0     0     0     0     0     0  1659     0     0  1353     1]]\n",
      "\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       171\n",
      "           1       0.00      0.00      0.00       420\n",
      "           2       0.00      0.00      0.00       334\n",
      "           3       0.00      0.00      0.00       686\n",
      "           4       0.00      0.00      0.00      1505\n",
      "           5       0.00      0.00      0.00       748\n",
      "           6       0.00      0.00      0.00       866\n",
      "           7       0.69      0.87      0.77     37206\n",
      "           8       0.00      0.00      0.00       225\n",
      "           9       1.00      0.01      0.03        77\n",
      "          10       0.51      0.48      0.49     18785\n",
      "          12       0.12      0.00      0.00      3013\n",
      "\n",
      "    accuracy                           0.64     64036\n",
      "   macro avg       0.19      0.11      0.11     64036\n",
      "weighted avg       0.56      0.64      0.59     64036\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id country\n",
       "0  5uwns89zht     NDF\n",
       "1  5uwns89zht      US\n",
       "2  5uwns89zht     nan\n",
       "3  5uwns89zht      IT\n",
       "4  5uwns89zht      FR"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.44933956e-03, 4.05452307e-03, 2.56952294e-03, 6.88130362e-03,\n",
       "        6.93166116e-03, 5.35723567e-03, 8.07176065e-03, 6.85570240e-01,\n",
       "        1.68390130e-03, 3.23357002e-04, 2.46408761e-01, 3.06983627e-02],\n",
       "       [5.94785379e-04, 1.54551340e-03, 9.97917028e-04, 2.17778399e-03,\n",
       "        5.09501947e-03, 2.87963334e-03, 3.15733487e-03, 8.92444134e-01,\n",
       "        3.89892200e-04, 1.49503991e-04, 7.27813616e-02, 1.77871604e-02],\n",
       "       [1.08308089e-03, 4.08504764e-03, 1.10268092e-03, 5.37870405e-03,\n",
       "        1.04722921e-02, 5.59335621e-03, 7.38951052e-03, 8.29451084e-01,\n",
       "        1.14177994e-03, 3.46786546e-04, 1.05490610e-01, 2.84650754e-02],\n",
       "       [9.19978600e-04, 2.65046698e-03, 6.36610959e-04, 3.81609565e-03,\n",
       "        6.32501021e-03, 3.48175457e-03, 4.85764444e-03, 9.12258446e-01,\n",
       "        7.37682683e-04, 3.63039871e-04, 4.70006168e-02, 1.69526022e-02],\n",
       "       [2.33743596e-03, 1.02233849e-02, 3.46002099e-03, 7.42188795e-03,\n",
       "        3.68361846e-02, 1.27262296e-02, 1.65517554e-02, 1.40754834e-01,\n",
       "        4.27195849e-03, 1.03601685e-03, 6.57326818e-01, 1.07053421e-01],\n",
       "       [1.35576108e-03, 8.96092784e-03, 3.75291705e-03, 9.67026036e-03,\n",
       "        2.05985215e-02, 1.13256536e-02, 2.10968461e-02, 3.03441048e-01,\n",
       "        2.46506836e-03, 1.26249297e-03, 5.54054677e-01, 6.20158203e-02],\n",
       "       [2.53824797e-03, 2.03704853e-02, 5.48941735e-03, 4.63068252e-03,\n",
       "        3.12358979e-02, 7.67711736e-03, 2.31726319e-02, 4.78555620e-01,\n",
       "        5.95656084e-03, 1.15834316e-03, 3.48042637e-01, 7.11723566e-02],\n",
       "       [1.09768042e-03, 3.92964436e-03, 1.95719092e-03, 5.95492031e-03,\n",
       "        1.46432789e-02, 7.52469199e-03, 8.97412933e-03, 8.38474452e-01,\n",
       "        1.57489162e-03, 1.09034264e-03, 8.77122432e-02, 2.70665251e-02],\n",
       "       [6.65175787e-04, 5.39582223e-03, 1.77861773e-03, 8.49382021e-03,\n",
       "        9.24407877e-03, 9.63453576e-03, 6.90033846e-03, 7.24072635e-01,\n",
       "        3.11648217e-03, 3.31084215e-04, 2.05959260e-01, 2.44081262e-02],\n",
       "       [7.65582779e-03, 6.43856125e-03, 1.86014792e-03, 1.39942840e-02,\n",
       "        3.73956859e-02, 9.76677146e-03, 1.64352264e-02, 2.20057249e-01,\n",
       "        5.10275876e-03, 2.50893296e-03, 5.87394238e-01, 9.13903043e-02],\n",
       "       [1.03407528e-03, 4.66343481e-03, 1.18409749e-03, 5.67959296e-03,\n",
       "        1.71572994e-02, 4.88854758e-03, 5.75504871e-03, 8.17361653e-01,\n",
       "        8.20520683e-04, 3.10500676e-04, 1.08923435e-01, 3.22218351e-02],\n",
       "       [1.38391508e-03, 9.17364284e-03, 3.77194257e-03, 9.37591866e-03,\n",
       "        2.64978223e-02, 7.05093984e-03, 1.32836811e-02, 4.52373981e-01,\n",
       "        5.46198711e-03, 7.33955123e-04, 4.23004180e-01, 4.78880778e-02],\n",
       "       [1.32664223e-03, 1.37258843e-02, 1.37086480e-03, 1.61092374e-02,\n",
       "        1.49595533e-02, 6.51463261e-03, 5.16518205e-03, 6.76485002e-01,\n",
       "        1.21252900e-02, 5.12614904e-04, 2.10610017e-01, 4.10950519e-02],\n",
       "       [3.92676768e-04, 2.90371710e-03, 2.94097839e-03, 8.40825122e-03,\n",
       "        1.02368770e-02, 4.39221133e-03, 3.69953923e-03, 8.61046433e-01,\n",
       "        7.61677627e-04, 1.16519681e-04, 8.37567896e-02, 2.13443488e-02],\n",
       "       [7.58177543e-04, 1.49486843e-03, 3.81587917e-04, 2.52270396e-03,\n",
       "        3.91469803e-03, 1.91662309e-03, 2.88415491e-03, 9.15197730e-01,\n",
       "        1.99343567e-03, 1.57820556e-04, 5.05786166e-02, 1.81995854e-02],\n",
       "       [2.55311839e-03, 8.70785490e-03, 1.04402835e-02, 2.00753137e-02,\n",
       "        4.01860215e-02, 1.49883013e-02, 1.28425099e-02, 3.57501566e-01,\n",
       "        9.97800101e-03, 5.67221374e-04, 4.00689393e-01, 1.21470422e-01],\n",
       "       [1.25340442e-03, 1.70652978e-02, 1.23015477e-03, 3.71198170e-03,\n",
       "        1.11568319e-02, 3.04971985e-03, 3.03678168e-03, 8.36150646e-01,\n",
       "        1.62797674e-04, 1.44800782e-04, 1.07146762e-01, 1.58908032e-02],\n",
       "       [1.78996602e-03, 1.20819202e-02, 1.33301097e-03, 9.45290551e-03,\n",
       "        1.65847745e-02, 1.04312459e-02, 8.71464703e-03, 3.66468310e-01,\n",
       "        2.58462294e-03, 4.48022533e-04, 4.79788572e-01, 9.03219953e-02],\n",
       "       [1.92247843e-03, 4.26541409e-03, 5.93469595e-04, 8.43786076e-03,\n",
       "        2.51159184e-02, 3.70488968e-03, 2.17110012e-02, 6.97955310e-01,\n",
       "        3.18256323e-03, 8.11262580e-04, 1.89538091e-01, 4.27617729e-02],\n",
       "       [1.05092756e-03, 7.30631547e-03, 5.93517302e-03, 9.90159716e-03,\n",
       "        1.01727080e-02, 5.26881358e-03, 8.20534676e-03, 6.38852119e-01,\n",
       "        1.82527711e-03, 7.60939845e-04, 2.50987291e-01, 5.97335063e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hold_pred[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the 5 classes with highest probabilities\n",
    "ids = []  #list of ids\n",
    "cts = []  #list of countries\n",
    "for i in range(len(test_ids)):\n",
    "    idx = test_ids[i]\n",
    "    ids += [idx] * 5\n",
    "    cts += le.inverse_transform(np.argsort(y_hold_pred[i])[::-1])[:5].tolist()\n",
    "\n",
    "# Generate submission\n",
    "sub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\n",
    "sub.country = sub.country.str.replace('nan','NDF')\n",
    "sub.to_csv('submission_29May_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-class Logistic Regression\n",
    "Note that regularization is applied by default as per the links:\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "2. https://stackoverflow.com/questions/31797149/l1-regularized-support-for-multinomial-logistic-regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "{'C': 0.1}\n",
      "0.5842117591941907\n",
      "0.5806109063651695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "model = LogisticRegression(penalty = \"l2\", multi_class=\"multinomial\", solver='lbfgs', max_iter=1000)\n",
    "parameters = {\"C\": [0.1, 1, 10]}\n",
    "fitmodel = GridSearchCV(model, param_grid=parameters, cv=5, scoring=\"accuracy\")\n",
    "fitmodel.fit(X_train, y_train.values.ravel())\n",
    "print(fitmodel.best_estimator_)\n",
    "print(fitmodel.best_params_)\n",
    "print(fitmodel.best_score_)\n",
    "\n",
    "print(accuracy_score(fitmodel.predict(X_test), y_test.values.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 150}\n",
      "Accuracy_score for train dataset 0.6850985510156277\n",
      "Accuracy_score for test dataset 0.49080204884752326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc = RandomForestClassifier(random_state = 42, class_weight=\"balanced\")\n",
    "param_grid = {'n_estimators': [150], 'max_features': ['sqrt'], 'max_depth' : [12, 15], 'criterion' :['gini', 'entropy']}  # 'max_depth' : [4,5,6,7,8], 'criterion' :['gini', 'entropy']\n",
    "# Random search of parameters\n",
    "CV_rfc = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 5)\n",
    "# Fit the model\n",
    "CV_rfc.fit(X_train, y_train.values.ravel())\n",
    "# print results\n",
    "print(CV_rfc.best_params_)\n",
    "print(\"Accuracy_score for train dataset\", accuracy_score(CV_rfc.predict(X_train), y_train))\n",
    "print(\"Accuracy_score for test dataset\", accuracy_score(CV_rfc.predict(X_test), y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
